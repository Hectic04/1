import numpy as np
def activfun_sigmoid(x, deriv=False):
   if deriv:
      return x * (1 - x)
   return 1 / (1 + np.exp( -x))


X = np.array([[1,1,0,1], [0,0,0,0], [0,1,1,1], [1,1,1,1], [0,1,0,1], [0,1,0,0], [1,0,0,1], [0,1,0,0]])


Y = np.array([[0, 0, 1, 0, 1, 0, 1, 0]]).T

np.random.seed(1)  # if you want same output in multiple executions


w0 = 2*np.random.random((4, 1)) -1

w0 = np.random.random((4,1))
'''
print("Initial weights -\n", w0)

for n in range(1200):
   l_input = X
   # feed forward
   l_output = activfun_sigmoid(l_input.dot(w0))  #h(x)

   
   l_output_error = Y - l_output # (y- h(x))
   l_output_delta = l_output_error * activfun_sigmoid(l_output, True) # (y - h(x)) * (h(x) * (1- h(x)))

   
   w0 = w0 + 2*l_input.T.dot(l_output_delta)


print("\nFinal weights -\n",w0)
print("\nOutout After Training:\n",l_output)
print("\nLoss: \n"+str(np.mean(np.square(Y - l_output))))

